{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e1da50-5d6d-4326-862e-6840b5458a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec55f8c8-8e8a-472d-90e6-74d098d217f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "  \"name\": \"John\",\n",
    "  \"age\": 30,\n",
    "  \"cars\": [\n",
    "    {\"car1\": \"Ford\", \"model\": \"Mustang\"},\n",
    "    {\"car2\": \"BMW\", \"model\": \"X5\"}\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a5fc64-a343-4b10-ad96-4b8ea2e7c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('nestedJSON').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8cc09ac-f6e8-4d74-a859-d8d987bc93c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('employee_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915c5509-db21-484a-9d30-3c82c55c7e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- candidate: struct (nullable = true)\n",
      " |    |    |    |-- experience: string (nullable = true)\n",
      " |    |    |    |-- first_name: string (nullable = true)\n",
      " |    |    |    |-- last_name: string (nullable = true)\n",
      " |    |    |    |-- relocation: string (nullable = true)\n",
      " |    |    |    |-- skills: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- specialty: string (nullable = true)\n",
      " |    |    |    |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e0b9ceb-cd92-4183-8485-f1c06904b1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           candidate|\n",
      "+--------------------+\n",
      "|{Mid, Margaret, M...|\n",
      "|{Senior, Michael,...|\n",
      "|{Mid, Brenda, Tyl...|\n",
      "|{Senior, Joseph, ...|\n",
      "|{Junior, Laura, W...|\n",
      "|{Mid, Cheryl, Ram...|\n",
      "|{Mid, Charles, St...|\n",
      "|{Senior, Bradley,...|\n",
      "|{Mid, William, Li...|\n",
      "|{Senior, Richard,...|\n",
      "|{Junior, Robert, ...|\n",
      "|{Mid, Tanya, Schu...|\n",
      "|{Senior, Scott, N...|\n",
      "|{Junior, Brett, H...|\n",
      "|{Junior, Sean, Wi...|\n",
      "|{Senior, Steven, ...|\n",
      "|{Mid, Mr., James,...|\n",
      "|{Junior, Linda, G...|\n",
      "|{Junior, Jacob, E...|\n",
      "|{Mid, Nicole, Mar...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flat = df.select(explode('features').alias('Elements'))\n",
    "df_flat = df_flat.select('Elements.candidate')\n",
    "\n",
    "df_flat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8202e5d-cbbe-46cf-a0c8-884e735177fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- candidate: struct (nullable = true)\n",
      " |    |-- experience: string (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |    |-- relocation: string (nullable = true)\n",
      " |    |-- skills: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- specialty: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f22a5dc3-4ec8-4215-ae3f-a02b9d1d07e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+----------+------------------+--------------------+-----+\n",
      "|first_name|last_name|experience|relocation|         specialty|              skills|state|\n",
      "+----------+---------+----------+----------+------------------+--------------------+-----+\n",
      "|  Margaret| Mcdonald|       Mid|        no|          Database|[skLearn, Java, R...|   AL|\n",
      "|   Michael|   Carter|    Senior|       yes|        Statistics|[TensorFlow, R, S...|   AR|\n",
      "|    Brenda|    Tyler|       Mid|        no|          Database|             [Spark]|   UT|\n",
      "|    Joseph|     King|    Senior|     maybe|  Machine Learning|[skLearn, SQL, R,...|   FL|\n",
      "|     Laura|     Webb|    Junior|     maybe|  Machine Learning|[TensorFlow, C++,...|   WY|\n",
      "|    Cheryl|  Ramirez|       Mid|        no|Data Visualization|[C++, Python, R, ...|   OK|\n",
      "|   Charles|  Stewart|       Mid|     maybe|  Machine Learning|[MongoDB, C++, Ja...|   NM|\n",
      "|   Bradley| Peterson|    Senior|       yes|Data Visualization|[skLearn, MongoDB...|   TX|\n",
      "|   William|      Lin|       Mid|     maybe|  Machine Learning|[Python, skLearn, R]|   WI|\n",
      "|   Richard|    Woods|    Senior|       yes|          Database|  [skLearn, MongoDB]|   UT|\n",
      "|    Robert|   Rivera|    Junior|       yes|          Database|[Java, skLearn, P...|   PA|\n",
      "|     Tanya|  Schultz|       Mid|       yes|        Statistics|[SQL, MongoDB, R,...|   NV|\n",
      "|     Scott|    Nunez|    Senior|     maybe|          Database|[skLearn, Java, S...|   MO|\n",
      "|     Brett|  Hoffman|    Junior|        no|          Database|[Java, TensorFlow...|   OK|\n",
      "|      Sean| Williams|    Junior|        no|Data Visualization|[SQL, MongoDB, R,...|   NV|\n",
      "|    Steven|    Craig|    Senior|     maybe|Data Visualization|   [TensorFlow, SQL]|   ID|\n",
      "|       Mr.|    James|       Mid|     maybe|Data Visualization|[TensorFlow, Java...|   KY|\n",
      "|     Linda|  Griffin|    Junior|        no|        Statistics|[TensorFlow, Spar...|   NE|\n",
      "|     Jacob| Espinoza|    Junior|     maybe|        Statistics|           [skLearn]|   WV|\n",
      "|    Nicole|   Martin|       Mid|     maybe|Data Visualization|         [Java, SQL]|   NC|\n",
      "+----------+---------+----------+----------+------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flat1=df.select(explode('features.candidate').alias('candi'))\n",
    "df_flat1 = df_flat1.select('candi.first_name','candi.last_name','candi.experience','candi.relocation','candi.specialty','candi.skills','candi.state')\n",
    "df_flat1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "516b6d22-4d24-46a6-8b38-58593bf2d4d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE] The CSV datasource doesn't support the column `skills` of the type \"ARRAY<STRING>\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_flat1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\conda\\envs\\jupyter-env\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\conda\\envs\\jupyter-env\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\conda\\envs\\jupyter-env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE] The CSV datasource doesn't support the column `skills` of the type \"ARRAY<STRING>\"."
     ]
    }
   ],
   "source": [
    "df_flat1.write.csv(\"output.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d9ba9dd-f1eb-4e38-9f61-bb11d78ae200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c2076df-c970-41de-9144-d561a7edfc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+----------+------------------+-----+--------------------+\n",
      "|first_name|last_name|experience|relocation|         specialty|state|          skills_str|\n",
      "+----------+---------+----------+----------+------------------+-----+--------------------+\n",
      "|  Margaret| Mcdonald|       Mid|        no|          Database|   AL|skLearn,Java,R,SQ...|\n",
      "|   Michael|   Carter|    Senior|       yes|        Statistics|   AR|TensorFlow,R,Spar...|\n",
      "|    Brenda|    Tyler|       Mid|        no|          Database|   UT|               Spark|\n",
      "|    Joseph|     King|    Senior|     maybe|  Machine Learning|   FL|skLearn,SQL,R,Spa...|\n",
      "|     Laura|     Webb|    Junior|     maybe|  Machine Learning|   WY|TensorFlow,C++,SQ...|\n",
      "|    Cheryl|  Ramirez|       Mid|        no|Data Visualization|   OK|C++,Python,R,Java...|\n",
      "|   Charles|  Stewart|       Mid|     maybe|  Machine Learning|   NM|MongoDB,C++,Java,...|\n",
      "|   Bradley| Peterson|    Senior|       yes|Data Visualization|   TX|skLearn,MongoDB,S...|\n",
      "|   William|      Lin|       Mid|     maybe|  Machine Learning|   WI|    Python,skLearn,R|\n",
      "|   Richard|    Woods|    Senior|       yes|          Database|   UT|     skLearn,MongoDB|\n",
      "|    Robert|   Rivera|    Junior|       yes|          Database|   PA|Java,skLearn,Pyth...|\n",
      "|     Tanya|  Schultz|       Mid|       yes|        Statistics|   NV|SQL,MongoDB,R,C++...|\n",
      "|     Scott|    Nunez|    Senior|     maybe|          Database|   MO|skLearn,Java,SQL,...|\n",
      "|     Brett|  Hoffman|    Junior|        no|          Database|   OK| Java,TensorFlow,C++|\n",
      "|      Sean| Williams|    Junior|        no|Data Visualization|   NV| SQL,MongoDB,R,Spark|\n",
      "|    Steven|    Craig|    Senior|     maybe|Data Visualization|   ID|      TensorFlow,SQL|\n",
      "|       Mr.|    James|       Mid|     maybe|Data Visualization|   KY|TensorFlow,Java,M...|\n",
      "|     Linda|  Griffin|    Junior|        no|        Statistics|   NE|TensorFlow,Spark,...|\n",
      "|     Jacob| Espinoza|    Junior|     maybe|        Statistics|   WV|             skLearn|\n",
      "|    Nicole|   Martin|       Mid|     maybe|Data Visualization|   NC|            Java,SQL|\n",
      "+----------+---------+----------+----------+------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flat1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2a1fd37-5e5a-45c1-9019-c32a9e757b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[{{Mid, Margaret,...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59117be5-db70-4f71-91a1-78da4a027901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Unset HADOOP_HOME environment variable\n",
    "os.environ.pop('HADOOP_HOME', None)\n",
    "\n",
    "# Save DataFrame as CSV using Pandas\n",
    "df_flat1.toPandas().to_csv('o.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763adf3-ac24-496a-b61f-f7ee8188bc60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
